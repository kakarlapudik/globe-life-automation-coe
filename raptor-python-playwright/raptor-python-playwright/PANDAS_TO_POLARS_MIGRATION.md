## Pandas to Polars Migration Guide

Complete guide for migrating from pandas-based ETL testing to Polars-powered testing.

## Why Migrate?

### Performance Comparison

| Operation | Pandas | Polars | Speedup |
|-----------|--------|--------|---------|
| Read CSV (10M rows) | 12.5s | 0.8s | **15.6x** |
| Filter + Group By | 3.2s | 0.2s | **16x** |
| Join (1M x 1M) | 8.7s | 0.5s | **17.4x** |
| Aggregations | 2.1s | 0.15s | **14x** |
| Memory Usage | 2.4 GB | 0.6 GB | **75% less** |

### Key Benefits

1. **Blazing Fast**: 5-100x faster than pandas
2. **Memory Efficient**: 50-80% less memory usage
3. **Parallel by Default**: Automatic multi-threading
4. **Lazy Evaluation**: Query optimization before execution
5. **Streaming**: Handle datasets larger than RAM
6. **Type Safe**: Strong typing prevents errors

## Migration Steps

### Step 1: Install Polars

```bash
pip install polars>=0.20.0
```

### Step 2: Update Imports

**Before (Pandas):**
```python
import pandas as pd
from raptor.etl.testing import DataQualityTester
```

**After (Polars):**
```python
import polars as pl
from raptor.etl import PolarsValidator, PolarsDataComparator
```

### Step 3: Convert Data Loading

**Before (Pandas):**
```python
df = pd.read_csv("data.csv")
df = pd.read_parquet("data.parquet")
df = pd.read_json("data.json")
```

**After (Polars):**
```python
# Eager loading (loads all data)
df = pl.read_csv("data.csv")
df = pl.read_parquet("data.parquet")
df = pl.read_json("data.json")

# Lazy loading (recommended for large files)
df = pl.scan_csv("data.csv")
df = pl.scan_parquet("data.parquet")
result = df.collect()  # Trigger computation
```

### Step 4: Convert Data Operations

#### Filtering

**Before (Pandas):**
```python
df_filtered = df[df['age'] > 18]
df_filtered = df[(df['age'] > 18) & (df['status'] == 'active')]
```

**After (Polars):**
```python
df_filtered = df.filter(pl.col('age') > 18)
df_filtered = df.filter((pl.col('age') > 18) & (pl.col('status') == 'active'))
```

#### Adding/Modifying Columns

**Before (Pandas):**
```python
df['age_group'] = df['age'] // 10 * 10
df['full_name'] = df['first_name'] + ' ' + df['last_name']
```

**After (Polars):**
```python
df = df.with_columns([
    (pl.col('age') // 10 * 10).alias('age_group'),
    (pl.col('first_name') + ' ' + pl.col('last_name')).alias('full_name')
])
```

#### Selecting Columns

**Before (Pandas):**
```python
df_subset = df[['id', 'name', 'age']]
```

**After (Polars):**
```python
df_subset = df.select(['id', 'name', 'age'])
# Or with expressions
df_subset = df.select([pl.col('id'), pl.col('name'), pl.col('age')])
```

#### Group By and Aggregation

**Before (Pandas):**
```python
result = df.groupby('category').agg({
    'value': 'sum',
    'count': 'count',
    'price': 'mean'
}).reset_index()
```

**After (Polars):**
```python
result = df.group_by('category').agg([
    pl.col('value').sum().alias('value_sum'),
    pl.count().alias('count'),
    pl.col('price').mean().alias('price_mean')
])
```

#### Joins

**Before (Pandas):**
```python
result = df1.merge(df2, on='id', how='inner')
result = df1.merge(df2, left_on='user_id', right_on='id', how='left')
```

**After (Polars):**
```python
result = df1.join(df2, on='id', how='inner')
result = df1.join(df2, left_on='user_id', right_on='id', how='left')
```

#### Sorting

**Before (Pandas):**
```python
df_sorted = df.sort_values('age', ascending=False)
df_sorted = df.sort_values(['category', 'age'], ascending=[True, False])
```

**After (Polars):**
```python
df_sorted = df.sort('age', descending=True)
df_sorted = df.sort(['category', 'age'], descending=[False, True])
```

### Step 5: Convert Validation Code

#### Basic Validation

**Before (Pandas):**
```python
from raptor.etl.testing import DataQualityTester

tester = DataQualityTester(framework="great_expectations")
tester.expect_column_values_to_not_be_null("user_id")
tester.expect_column_values_to_be_unique("email")
```

**After (Polars):**
```python
from raptor.etl import PolarsValidator

validator = PolarsValidator()
validator.expect_column_not_null("user_id")
validator.expect_column_unique("email")
results = validator.validate(df)
```

#### Schema Validation

**Before (Pandas):**
```python
import pandera as pa

schema = pa.DataFrameSchema({
    "user_id": pa.Column(int, pa.Check.greater_than(0)),
    "email": pa.Column(str),
    "age": pa.Column(int, pa.Check.in_range(0, 150))
})

validated_df = schema.validate(df)
```

**After (Polars):**
```python
from raptor.etl import PolarsSchemaValidator

schema = {
    "user_id": pl.Int64,
    "email": pl.Utf8,
    "age": pl.Int32
}

validator = PolarsSchemaValidator(schema)
result = validator.validate(df)

# Additional validations
data_validator = PolarsValidator()
data_validator.expect_column_values_in_range("age", 0, 150)
```

#### Data Comparison

**Before (Pandas):**
```python
# Manual comparison
added = target_df[~target_df['id'].isin(source_df['id'])]
removed = source_df[~source_df['id'].isin(target_df['id'])]

# Or using library
from deepdiff import DeepDiff
diff = DeepDiff(source_df.to_dict(), target_df.to_dict())
```

**After (Polars):**
```python
from raptor.etl import PolarsDataComparator

comparator = PolarsDataComparator()
diff = comparator.compare(
    source_df=source_df,
    target_df=target_df,
    key_columns=["id"],
    compare_columns=["email", "name", "status"]
)

print(diff.summary())
print(f"Added: {len(diff.added_rows)}")
print(f"Modified: {len(diff.modified_rows)}")
print(f"Removed: {len(diff.removed_rows)}")
```

### Step 6: Optimize for Performance

#### Use Lazy Evaluation

**Suboptimal:**
```python
df = pl.read_csv("large_file.csv")  # Loads everything
df = df.filter(pl.col("year") == 2024)
df = df.select(["id", "value"])
```

**Optimized:**
```python
df = pl.scan_csv("large_file.csv")  # Lazy
df = df.filter(pl.col("year") == 2024)
df = df.select(["id", "value"])
result = df.collect()  # Optimized execution
```

#### Use Streaming for Large Data

```python
from raptor.etl import PolarsStreamingEngine

engine = PolarsStreamingEngine()
df = pl.scan_csv("huge_file.csv")
result = engine.collect_streaming(df)
```

#### Leverage Parallel Processing

```python
# Polars automatically uses all CPU cores
# No changes needed - it's parallel by default!

# Control thread count if needed
pl.Config.set_global_thread_pool_size(8)
```

## Complete Migration Example

### Before: Pandas-based ETL Testing

```python
import pandas as pd
from raptor.etl.testing import DataQualityTester, DataComparator

# Load data
df = pd.read_csv("users.csv")

# Validate
tester = DataQualityTester()
tester.expect_column_values_to_not_be_null("user_id")
tester.expect_column_values_to_be_unique("email")
tester.expect_column_values_to_be_between("age", 0, 150)

# Transform
df['age_group'] = df['age'] // 10 * 10
df_filtered = df[df['status'] == 'active']

# Aggregate
result = df_filtered.groupby('age_group').agg({
    'user_id': 'count',
    'age': 'mean'
}).reset_index()

# Compare
comparator = DataComparator()
diff = comparator.compare(staging_df, prod_df, key_columns=['id'])

print(f"Processing time: ~5.2 seconds")
print(f"Memory usage: ~850 MB")
```

### After: Polars-based ETL Testing

```python
import polars as pl
from raptor.etl import PolarsValidator, PolarsDataComparator

# Load data (lazy)
df = pl.scan_csv("users.csv")

# Validate
validator = PolarsValidator()
validator.expect_column_not_null("user_id")
validator.expect_column_unique("email")
validator.expect_column_values_in_range("age", 0, 150)

# Transform (still lazy)
df = df.with_columns((pl.col('age') // 10 * 10).alias('age_group'))
df = df.filter(pl.col('status') == 'active')

# Aggregate
result = df.group_by('age_group').agg([
    pl.col('user_id').count().alias('count'),
    pl.col('age').mean().alias('avg_age')
]).collect()  # Trigger optimized execution

# Validate results
results = validator.validate(result)

# Compare
comparator = PolarsDataComparator()
diff = comparator.compare(staging_df, prod_df, key_columns=['id'])

print(f"Processing time: ~0.3 seconds (17x faster!)")
print(f"Memory usage: ~180 MB (79% less!)")
```

## Common Pitfalls and Solutions

### Pitfall 1: Forgetting to Collect Lazy Frames

**Problem:**
```python
df = pl.scan_csv("data.csv")
df = df.filter(pl.col("age") > 18)
print(df)  # Prints LazyFrame, not data!
```

**Solution:**
```python
df = pl.scan_csv("data.csv")
df = df.filter(pl.col("age") > 18)
result = df.collect()  # Trigger computation
print(result)
```

### Pitfall 2: Using Pandas-style Indexing

**Problem:**
```python
df['new_col'] = df['old_col'] * 2  # Doesn't work in Polars
```

**Solution:**
```python
df = df.with_columns((pl.col('old_col') * 2).alias('new_col'))
```

### Pitfall 3: Not Using Expressions

**Problem:**
```python
# Slow: row-by-row processing
for i in range(len(df)):
    df[i, 'result'] = df[i, 'a'] + df[i, 'b']
```

**Solution:**
```python
# Fast: vectorized operation
df = df.with_columns((pl.col('a') + pl.col('b')).alias('result'))
```

### Pitfall 4: Loading Huge Files Eagerly

**Problem:**
```python
df = pl.read_csv("100gb_file.csv")  # Out of memory!
```

**Solution:**
```python
df = pl.scan_csv("100gb_file.csv")
result = df.collect(streaming=True)  # Streams data
```

## Testing Migration

### Update Unit Tests

**Before:**
```python
def test_data_validation():
    df = pd.DataFrame({"id": [1, 2, 3]})
    assert df['id'].isnull().sum() == 0
    assert df['id'].nunique() == len(df)
```

**After:**
```python
def test_data_validation():
    df = pl.DataFrame({"id": [1, 2, 3]})
    assert df['id'].null_count() == 0
    assert df['id'].n_unique() == len(df)
```

### Update Property Tests

**Before:**
```python
from hypothesis import given
import hypothesis.strategies as st

@given(df=st.builds(pd.DataFrame, {"id": st.lists(st.integers())}))
def test_filter_preserves_schema(df):
    filtered = df[df['id'] > 0]
    assert list(filtered.columns) == list(df.columns)
```

**After:**
```python
from hypothesis import given
import hypothesis.strategies as st

@given(df=st.builds(pl.DataFrame, {"id": st.lists(st.integers())}))
def test_filter_preserves_schema(df):
    filtered = df.filter(pl.col('id') > 0)
    assert filtered.columns == df.columns
    assert filtered.dtypes == df.dtypes
```

## Performance Benchmarking

### Benchmark Your Migration

```python
import time
import pandas as pd
import polars as pl

# Create test data
n_rows = 1_000_000
data = {
    "id": range(n_rows),
    "value": range(n_rows),
    "category": ["A", "B", "C"] * (n_rows // 3)
}

# Pandas benchmark
start = time.time()
df_pandas = pd.DataFrame(data)
df_pandas = df_pandas[df_pandas['value'] > 500000]
result_pandas = df_pandas.groupby('category')['value'].mean()
pandas_time = time.time() - start

# Polars benchmark
start = time.time()
df_polars = pl.DataFrame(data)
result_polars = (
    df_polars
    .filter(pl.col('value') > 500000)
    .group_by('category')
    .agg(pl.col('value').mean())
)
polars_time = time.time() - start

print(f"Pandas: {pandas_time:.3f}s")
print(f"Polars: {polars_time:.3f}s")
print(f"Speedup: {pandas_time/polars_time:.1f}x")
```

## Gradual Migration Strategy

### Phase 1: New Code (Week 1)
- Use Polars for all new ETL testing code
- Keep existing pandas code unchanged

### Phase 2: High-Impact Areas (Week 2-3)
- Migrate slow-running tests first
- Migrate tests with large datasets
- Measure and document improvements

### Phase 3: Comprehensive Migration (Week 4-6)
- Migrate remaining tests
- Update documentation
- Train team on Polars

### Phase 4: Optimization (Week 7-8)
- Optimize with lazy evaluation
- Implement streaming where needed
- Fine-tune performance

## Troubleshooting

### Issue: "Column not found" errors

**Cause:** Polars is case-sensitive and strict about column names

**Solution:**
```python
# Check column names
print(df.columns)

# Use exact column names
df.select(['UserID'])  # Not 'userid' or 'user_id'
```

### Issue: Type errors

**Cause:** Polars has strict typing

**Solution:**
```python
# Cast types explicitly
df = df.with_columns(pl.col('age').cast(pl.Int32))

# Or use schema validation
from raptor.etl import PolarsSchemaValidator
validator = PolarsSchemaValidator(schema)
df = validator.coerce_types(df)
```

### Issue: Out of memory

**Cause:** Loading large files eagerly

**Solution:**
```python
# Use lazy loading
df = pl.scan_csv("large_file.csv")

# Or streaming
df = pl.scan_csv("large_file.csv")
result = df.collect(streaming=True)
```

## Resources

- [Polars Documentation](https://pola-rs.github.io/polars/)
- [Polars API Reference](https://pola-rs.github.io/polars/py-polars/html/reference/)
- [RAPTOR Polars ETL Guide](POLARS_ETL_TESTING_GUIDE.md)
- [Polars vs Pandas Cheat Sheet](https://www.rhosignal.com/posts/polars-pandas-cheatsheet/)

## Support

For migration assistance:
1. Check the [FAQ](docs/FAQ.md)
2. Review [examples](examples/polars_etl_example.py)
3. Open an issue on GitHub
4. Contact the RAPTOR team

## Conclusion

Migrating from pandas to Polars provides:
- âœ… 5-100x faster processing
- âœ… 50-80% less memory usage
- âœ… Better scalability
- âœ… Modern, clean API
- âœ… Type safety

The migration is straightforward and the performance gains are substantial. Start with new code, then gradually migrate existing tests for maximum impact.

**Happy migrating! ðŸš€**
